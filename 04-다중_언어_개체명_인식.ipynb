{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c3970f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/nlp-with-transformers/blob/main/04-다중_언어_개체명_인식.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c981737",
   "metadata": {},
   "source": [
    "#  텍스트 분류 목차\n",
    "* [Chapter 0 개요](#chapter0)\n",
    "* [Chapter 1 데이터셋](#chapter1)\n",
    "* [Chapter 2 다중 언어 트랜스포머](#chapter2)\n",
    "* [Chapter 3 XLM-R 토큰화](#chapter3)\n",
    "    * [Chapter 3-1 토큰화 파이프라인](#chapter3-1)\n",
    "    * [Chapter 3-2 SentencePiece 토크나이저](#chapter3-2)\n",
    "* [Chapter 4 개체명 인식을 위한 트랜스포머](#chapter4)    \n",
    "* [Chapter 5 트랜스포머 모델 클래스](#chapter5)    \n",
    "    * [Chapter 5-1 토큰 분류를 위한 사용자 정의 모델 만들기](#chapter5-1)    \n",
    "    * [Chapter 5-2 사용자 정의 모델 로드하기](#chapter5-2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37206bdc",
   "metadata": {},
   "source": [
    "## Chapter 0 개요 <a class=\"anchor\" id=\"chapter0\"></a>\n",
    "1. NLP를 적용할 문서가 다른 언어이거나, 다국어로 되어있는 경우 문제가 발생할 수 있다.\n",
    "    - 러시아어, 중국어, 독일어와 같은 대표적인 언어는 허깅페이스에서 적절한 사전 훈련된 언어 모델을 찾아 미세튜닝 가능하다.\n",
    "    - 그리스어, 스와힐리어 등의 언어는 사전 훈련된 모델을 찾기 어려울 수 있다.\n",
    "\n",
    "2. 다중 언어 트랜스포머 모델이 등장하면서, 하나의 모델로 여러 언어를 처리할 수 있게 되었다.\n",
    "    - 대표적인 다중 언어 트랜스포머 모델: mBERT, XLM-RoBERTa\n",
    "    - 다중 언어 트랜스포머 모델은 여러 언어로 된 대규모 말뭉치를 사용해 사전 훈련되었다.\n",
    "    - 다중 언어 트랜스포머 모델은 단일 언어 모델에 비해 성능이 약간 떨어질 수 있지만, 다양한 언어를 처리할 수 있다는 장점이 있다.\n",
    "\n",
    "3. 다중 언어 트랜스포머는 많은 언어로 된 대규모 말뭉치에서 사전 훈련해서 제로샷 교차 언어 전이가 가능하다.\n",
    "    - 제로샷 교차 언어 전이: 모델이 훈련되지 않은 언어로 된 문서에 대해 예측할 수 있는 능력\n",
    "    - 예를 들어, 영어로 된 데이터셋으로 미세튜닝한 모델이 러시아어로 된 문서에 대해서도 예측할 수 있다.\n",
    "\n",
    "4. 이번 장에서는 XML-RoBERTa 모델을 사용해 다중 언어 개체명 인식(NER) 작업을 수행한다.\n",
    "    - 개체명 인식(NER): 문서에서 사람, 장소, 조직 등의 개체를 식별하는 작업\n",
    "    - 다중 언어 NER 작업에서는 여러 언어로 된 문서에서 개체를 식별해야 한다.\n",
    "    - 예를 들어 회사 문서에서 중요한 정보를 추출하거나, 검색 엔진의 품직을 높이거나, 말뭉치에서 구조적인 데이터를 생성하는 데 활용할 수 있다.\n",
    "\n",
    "5. 네 개의 공용어를 사용하여 스위스에서 주로 활동하는 고객을 위해 NER을 수행한다고 가정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01753b9d",
   "metadata": {},
   "source": [
    "## Chapter 1 데이터셋 <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "1. WikiANN 또는 APN-X라 불리는 교차 언어 전이 평가 벤치마크 데이터를 사용한다.\n",
    "    - 여러 언어의 위키피디피아 문서로 구성되고 스위스에서 사용되는 독일어(62.9%), 프랑스어(22.9%), 이탈리아어(8.4%), 영어(5.9%) 네 개 언어로 작성됐다.\n",
    "    - 각 문서는 IOB2 포멧으로 LOC(위치), PER(사람), ORG(조직) 세 가지 개체 유형으로 주석이 달려 있다.\n",
    "        - B- 접두사: 개체명의 시작을 나타낸다.\n",
    "        - I- 접두사: 동일한 개체명에 속해 연속되는 토큰을 나타낸다.\n",
    "        - O: 개체명에 속하지 않는 토큰을 나타낸다.\n",
    "    - 예를 그림으로 표현하면 아래와 같다.\n",
    "\n",
    "         ![WikiANN 예시](image/04_01_WikiANN.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb5e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME 서브셋 개수: 183\n",
      "['MLQA.ar.ar', 'MLQA.ar.de', 'MLQA.ar.en', 'MLQA.ar.es', 'MLQA.ar.hi', 'MLQA.ar.vi', 'MLQA.ar.zh', 'MLQA.de.ar', 'MLQA.de.de', 'MLQA.de.en', 'MLQA.de.es', 'MLQA.de.hi', 'MLQA.de.vi', 'MLQA.de.zh', 'MLQA.en.ar', 'MLQA.en.de', 'MLQA.en.en', 'MLQA.en.es', 'MLQA.en.hi', 'MLQA.en.vi', 'MLQA.en.zh', 'MLQA.es.ar', 'MLQA.es.de', 'MLQA.es.en', 'MLQA.es.es', 'MLQA.es.hi', 'MLQA.es.vi', 'MLQA.es.zh', 'MLQA.hi.ar', 'MLQA.hi.de', 'MLQA.hi.en', 'MLQA.hi.es', 'MLQA.hi.hi', 'MLQA.hi.vi', 'MLQA.hi.zh', 'MLQA.vi.ar', 'MLQA.vi.de', 'MLQA.vi.en', 'MLQA.vi.es', 'MLQA.vi.hi', 'MLQA.vi.vi', 'MLQA.vi.zh', 'MLQA.zh.ar', 'MLQA.zh.de', 'MLQA.zh.en', 'MLQA.zh.es', 'MLQA.zh.hi', 'MLQA.zh.vi', 'MLQA.zh.zh', 'PAN-X.af', 'PAN-X.ar', 'PAN-X.bg', 'PAN-X.bn', 'PAN-X.de', 'PAN-X.el', 'PAN-X.en', 'PAN-X.es', 'PAN-X.et', 'PAN-X.eu', 'PAN-X.fa', 'PAN-X.fi', 'PAN-X.fr', 'PAN-X.he', 'PAN-X.hi', 'PAN-X.hu', 'PAN-X.id', 'PAN-X.it', 'PAN-X.ja', 'PAN-X.jv', 'PAN-X.ka', 'PAN-X.kk', 'PAN-X.ko', 'PAN-X.ml', 'PAN-X.mr', 'PAN-X.ms', 'PAN-X.my', 'PAN-X.nl', 'PAN-X.pt', 'PAN-X.ru', 'PAN-X.sw', 'PAN-X.ta', 'PAN-X.te', 'PAN-X.th', 'PAN-X.tl', 'PAN-X.tr', 'PAN-X.ur', 'PAN-X.vi', 'PAN-X.yo', 'PAN-X.zh', 'PAWS-X.de', 'PAWS-X.en', 'PAWS-X.es', 'PAWS-X.fr', 'PAWS-X.ja', 'PAWS-X.ko', 'PAWS-X.zh', 'SQuAD', 'XNLI', 'XQuAD.ar', 'XQuAD.de', 'XQuAD.el', 'XQuAD.en', 'XQuAD.es', 'XQuAD.hi', 'XQuAD.ru', 'XQuAD.th', 'XQuAD.tr', 'XQuAD.vi', 'XQuAD.zh', 'bucc18.de', 'bucc18.fr', 'bucc18.ru', 'bucc18.zh', 'tatoeba.afr', 'tatoeba.ara', 'tatoeba.ben', 'tatoeba.bul', 'tatoeba.cmn', 'tatoeba.deu', 'tatoeba.ell', 'tatoeba.est', 'tatoeba.eus', 'tatoeba.fin', 'tatoeba.fra', 'tatoeba.heb', 'tatoeba.hin', 'tatoeba.hun', 'tatoeba.ind', 'tatoeba.ita', 'tatoeba.jav', 'tatoeba.jpn', 'tatoeba.kat', 'tatoeba.kaz', 'tatoeba.kor', 'tatoeba.mal', 'tatoeba.mar', 'tatoeba.nld', 'tatoeba.pes', 'tatoeba.por', 'tatoeba.rus', 'tatoeba.spa', 'tatoeba.swh', 'tatoeba.tam', 'tatoeba.tel', 'tatoeba.tgl', 'tatoeba.tha', 'tatoeba.tur', 'tatoeba.urd', 'tatoeba.vie', 'tydiqa', 'udpos.Afrikaans', 'udpos.Arabic', 'udpos.Basque', 'udpos.Bulgarian', 'udpos.Chinese', 'udpos.Dutch', 'udpos.English', 'udpos.Estonian', 'udpos.Finnish', 'udpos.French', 'udpos.German', 'udpos.Greek', 'udpos.Hebrew', 'udpos.Hindi', 'udpos.Hungarian', 'udpos.Indonesian', 'udpos.Italian', 'udpos.Japanese', 'udpos.Kazakh', 'udpos.Korean', 'udpos.Marathi', 'udpos.Persian', 'udpos.Portuguese', 'udpos.Russian', 'udpos.Spanish', 'udpos.Tagalog', 'udpos.Tamil', 'udpos.Telugu', 'udpos.Thai', 'udpos.Turkish', 'udpos.Urdu', 'udpos.Vietnamese', 'udpos.Yoruba']\n"
     ]
    }
   ],
   "source": [
    "# TMREME에서 PAN-X 서브셋 중 하나를 로드한다.\n",
    "#   - load_dataset() 함수에 전달할 이름을 확인하기 위해 get_dataset_config_names() 함수를 사용한다.\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME 서브셋 개수: {len(xtreme_subsets)}\")\n",
    "print(xtreme_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e1787b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAN-X 서브셋 개수: 40\n",
      "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg', 'PAN-X.bn', 'PAN-X.de', 'PAN-X.el', 'PAN-X.en', 'PAN-X.es', 'PAN-X.et', 'PAN-X.eu', 'PAN-X.fa', 'PAN-X.fi', 'PAN-X.fr', 'PAN-X.he', 'PAN-X.hi', 'PAN-X.hu', 'PAN-X.id', 'PAN-X.it', 'PAN-X.ja', 'PAN-X.jv', 'PAN-X.ka', 'PAN-X.kk', 'PAN-X.ko', 'PAN-X.ml', 'PAN-X.mr', 'PAN-X.ms', 'PAN-X.my', 'PAN-X.nl', 'PAN-X.pt', 'PAN-X.ru', 'PAN-X.sw', 'PAN-X.ta', 'PAN-X.te', 'PAN-X.th', 'PAN-X.tl', 'PAN-X.tr', 'PAN-X.ur', 'PAN-X.vi', 'PAN-X.yo', 'PAN-X.zh']\n"
     ]
    }
   ],
   "source": [
    "# \"PAN\"으로 시작하는 서브셋을 찾는다.\n",
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "print(f\"PAN-X 서브셋 개수: {len(panx_subsets)}\")\n",
    "print(panx_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066dafab",
   "metadata": {},
   "source": [
    "2. ISO 639-1 언어 코드로 보이는 두 문자로 된 접미사가 있다.\n",
    "    - de: 독일어\n",
    "    - fr: 프랑스어\n",
    "    - it: 이탈리아어\n",
    "    - en: 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8112f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "panx_de = load_dataset(\"xtreme\", \"PAN-X.de\")\n",
    "print(panx_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3a707",
   "metadata": {},
   "source": [
    "3. 독일어, 프랑스어, 이탈리아어, 영어 말뭉치를 샘를링한다.\n",
    "    - 분균형한 데이터셋이 만들어지는데, 실제 데이터셋에서는 흔히 벌어지는 일이다.\n",
    "    - 소수 언어에서 레이블링된 샘플을 구하려면 비용이 많이 들기도 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7480973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]  # PAN-X 언어별 샘플 비율\n",
    "\n",
    "panx_ch = defaultdict(DatasetDict) # 키가 없는 경우 빈 DatasetDict 생성\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # 다국어 말뭉치를 로드합니다.\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # 각 분할을 언어 비율에 따라 다운샘플링하고 섞습니다.\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * ds[split].num_rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68330fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>훈련 샘플 수</th>\n",
       "      <td>12580</td>\n",
       "      <td>4580</td>\n",
       "      <td>1680</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            de    fr    it    en\n",
       "훈련 샘플 수  12580  4580  1680  1180"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 세트에 언어마다 얼마나 많은 샘플이 들어있는지 확인한다.\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({lang: panx_ch[lang][\"train\"].num_rows for lang in langs}, index=[\"훈련 샘플 수\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accecf5e",
   "metadata": {},
   "source": [
    "4. 독일어 샘플은 그 외 언어를 모두 합친 것보다 더 많다.\n",
    "\n",
    "5. 이 데이터셋을 사용해 제로샷 교차 언어 전이를 프랑스어, 이탈리아어, 영어에 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1384694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n",
      "ner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "source": [
    "# 독일어 말뭉치에 있는 샘플 한 개 확인\n",
    "#   - 샘플의 키는 애로우 테이블의 열 이름에 해당하고 값은 각 열에 있는 항목이다.\n",
    "#   - ner_tags 열은 각 개체면에 매핑된 클래스 ID에 해당한다.\n",
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb83a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: List(Value('string'))\n",
      "ner_tags: List(ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']))\n",
      "langs: List(Value('string'))\n"
     ]
    }
   ],
   "source": [
    "# 이해하기 쉽게 LOC, PER, ORG 태크로 새로운 열을 만든다.\n",
    "#   - Dataset 객체는 각 열의 데이터 타입을 담은 feature 속성을 가진다.\n",
    "#   - nert_tags 열은 ClassLable의 리스트이다.\n",
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a2b2188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'])\n"
     ]
    }
   ],
   "source": [
    "# 훈련 세트에서 특성을 확인한다\n",
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e551fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c7d377782c4d3ca45e45e66d504f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d437e8ed5fd249bd9e7278eff610fc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa905334403404c8c92b1e66268e907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# int2str() 메서드를 사용해 클래스 ID를 레이블 문자열로 변환한다.\n",
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(i) for i in batch[\"ner_tags\"]]}\n",
    "\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29f07d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>토큰</th>\n",
       "      <td>2.000</td>\n",
       "      <td>Einwohnern</td>\n",
       "      <td>an</td>\n",
       "      <td>der</td>\n",
       "      <td>Danziger</td>\n",
       "      <td>Bucht</td>\n",
       "      <td>in</td>\n",
       "      <td>der</td>\n",
       "      <td>polnischen</td>\n",
       "      <td>Woiwodschaft</td>\n",
       "      <td>Pommern</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>태그</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0           1   2    3   ...          8             9        10 11\n",
       "토큰  2.000  Einwohnern  an  der  ...  polnischen  Woiwodschaft  Pommern  .\n",
       "태그      O           O   O    O  ...       B-LOC         B-LOC    I-LOC  O\n",
       "\n",
       "[2 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 샘플의 토큰과 태그 이름을 나란히 출력한다.\n",
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]], index=[\"토큰\", \"태그\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b5b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>6186</td>\n",
       "      <td>5366</td>\n",
       "      <td>5810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>3172</td>\n",
       "      <td>2683</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3180</td>\n",
       "      <td>2573</td>\n",
       "      <td>3071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       6186  5366  5810\n",
       "validation  3172  2683  2893\n",
       "test        3180  2573  3071"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태그가 분균형하게 부여되지 않았나 확인하기 위해 각 분할에서 개체면의 빈도를 계산한다.\n",
    "#  - B-LOC, B-PER, B-ORG 태그로 시작하는 태그의 개수를 센다.\n",
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c3fac",
   "metadata": {},
   "source": [
    "## Chapter 2 다중 언어 트랜스포머 <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "1. 다중 언어 트랜스포머의 훈련 과정과 아키텍쳐는 단일 언어 트랜스포머와 비슷하다.\n",
    "    - 다중 언어 트랜스포머는 여러 언어로 된 대규모 말뭉치를 사용해 사전 훈련된다.\n",
    "    - 예를 들어, mBERT는 104개 언어로 된 위키피디피아 문서를 사용해 사전 훈련되었다.\n",
    "    - XLM-RoBERTa는 100개 언어로 된 CommonCrawl 데이터를 사용해 사전 훈련되었다.\n",
    "\n",
    "2. NER에 대한 교차 언어 전이의 과정을 측정하기 위해 CoNLL-2002와 CoNLL-2003 데이셋이 영어, 네덜란드어, 스페인어, 독일어를 위한 벤치 마크로 자주 사용된다.\n",
    "    - 이 벤치마크틑 PAN-X와 동일하게 개체명이 LOC, PER, ORG 태그로 분류된 뉴스 기사로 구성된다.\n",
    "    - 세 카타고리에 속하지 않은 개체명을 위해 MISC 태그도 포함된다.\n",
    "\n",
    "3. 다중 언어 트랜스포머 모델은 일반적으로 세 가지 방식으로 평가한다.\n",
    "    - en: 영어 훈련 데이터에서 미세 튜닝한 다음에 각 언어의 테스트 세트에서 평가한다.\n",
    "    - each: 언어별 성능을 측정하기 위해 단일 언어의 테스트 세트에서 미세 튜닝하고 평가한다.\n",
    "    - all: 모든 언어의 훈련 데이터를 결합해 미세 튜닝한 다음에 각 언어의 테스트 세트에서 평가한다.\n",
    "\n",
    "4. NER 평가 모델로 XLM-RoBERTa 모델을 사용한다.\n",
    "    - XLM-RoBERTa는 사전 훈련 방식이 단일 언어 모델 RoBERTa와 동일한 다중 언어 트랜스포머 모델이다.\n",
    "    - 100개 언어로 된 CommonCrawl 데이터를 사용해 사전 훈련되었다.\n",
    "    - XLM에서 사용하는 언어 임베딩을 사용하지 않고, 단일 어휘 집합을 사용해 토큰화한다.\n",
    "        - 따라서 동일한 단어의 토큰은 동일한 임베딩 ID를 갖는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf54931",
   "metadata": {},
   "source": [
    "## Chapter 3 XLM-R 토큰화 <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "1. XLM-R은 WordPice 토크나이져 대신 100개의 언어 텍스트에서 훈련된 SentencePiece 토크나이져를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a74c5985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/redinblue/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/redinblue/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/redinblue/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/redinblue/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/redinblue/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/redinblue/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/redinblue/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /home/redinblue/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/redinblue/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/redinblue/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SentencePice 토크나이져와 WordPiece 토크나이져 비교를 위해 토크나이져 로드\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\" # BERT 모델 이름\n",
    "xlmr_model_name = \"xlm-roberta-base\" # XLM-R 모델 이름\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name) # BERT 토크나이져\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name) # XLM-R 토크나이져"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2594ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']\n",
      "['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# 짧은 텍스트 시퀸스를 인코딩해서 각 모델이 사전 훈련 동안에 사용하는 특수 토큰 확인\n",
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "print(bert_tokens)\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "print(xlmr_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3ce43",
   "metadata": {},
   "source": [
    "### Chapter 3-1 토큰화 파이프라인 <a class=\"anchor\" id=\"chapter3-1\"></a>\n",
    "1. 토큰화는 아래의 이미지와 같은 네 단계의 파이프라인으로 구성된다.\n",
    "\n",
    "     ![XLM-R 토큰화 파이프라인](image/04_03_XLMR_Tokenization_Pipeline.png)\n",
    "\n",
    "2. 정규화\n",
    "     - 원시 문자역을 더 \"깨긋하게\" 만들기 위해 적용하는 일련의 연산이다.\n",
    "     - 예를 들어, 대문자를 소문자로 변환하거나, 악센트 부호를 제거하거나, 특수 문자를 처리하는 작업이 포함된다.\n",
    "     - 유니코드 정규화는 많은 토크나이저에서 적용되는 또 다른 일반적인 정규화 연산이며, 같은 문자를 쓰는 여러 가지 방식을 처리한다.\n",
    "          - 같은 문자열의 두 버전이 다르게 표시될 수도 있다.\n",
    "          - 예를 들어, \"é\" 문자는 단일 문자로 표현될 수도 있고, \"e\" 문자와 악센트 부호로 분리되어 표현될 수도 있다.\n",
    "\n",
    "3. 사전 토큰화\n",
    "     - 텍스트를 더 작은 객체로 분활하며 훈련 마지막에 생성되는 토큰의 상한선을 제공한다.\n",
    "     - 사전 토큰화가 텍스트를 단어로 분할하고 최종 토큰은 이 단어의 일부가 되다.\n",
    "     - 예를 들어, \"Jack Sparrow loves New York!\" 문장은 공백과 특수문자를 기준으로 분할되어 여섯 개로 나뉜다.\n",
    "          - [\"Jack\", \"Sparrow\", \"loves\", \"New\", \"York\", \"!\"]\n",
    "     - 이 단어들은 다음단계에서 BPE나 유니그램 알고리즘을 상용해서 더 작은 단위로 분할된다.\n",
    "\n",
    "4. 토크나이져 모델\n",
    "     - 입력 텍스트 정규화와 사전 토큰화를 수행하고 난 후 토크나이져를 사용해 부분단어 분할 모델을 단어에 적용한다.\n",
    "     - 토크나이져는 파이프라인에서 말뭉치로 훈련이 필요한 부분이다.\n",
    "     - 단어를 부분단어로 나눠 어휘사전의 크기와 OOV 토큰의 개수를 출이는 역활을 한다.\n",
    "     - 예시 문장에 토크나이져 모델을 적용하면 다음과 같은 토큰이 생성된다.\n",
    "          - [\"jack\", \"spa\", \"rrow\", \"love\", \"s\", \"new\", \"york\", \"!\"]\n",
    "     - 이 시점부터 더 이상 문자열 리스트가 아니라 정수(입력ID) 리스트를 가지게 된다.\n",
    "\n",
    "5. 사후 처리\n",
    "     - 토큰 리스트에 부가적인 변환을 적용한다.\n",
    "     - 예를 들어, 모델에 입력하기 위해 특수 토큰을 추가하거나, 토큰 리스트를 고정된 길이로 패딩하는 작업이 포함된다.\n",
    "     - BERT 모델에서는 [CLS] 토큰을 문장 맨 앞에 추가하고, [SEP] 토큰을 문장 맨 뒤에 추가한다.\n",
    "     - XLM-R 모델에서는 <s> 토큰을 문장 맨 앞에 추가하고, </s> 토큰을 문장 맨 뒤에 추가한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1f628",
   "metadata": {},
   "source": [
    "### Chapter 3-2 SentencePiece 토크나이저 <a class=\"anchor\" id=\"chapter3-2\"></a>\n",
    "1. SentencePiece 토크나이저는 유니그램이라는 부분단어 분할 방식을 기반으로 각 입력 텍스트를 유니코드 문자 시퀸스로 인코딩한다.\n",
    "    - 악센트, 구두점에 대해 몰라도 되므로 이 특징은 다국어 말뭉치에 특히 유용하다.\n",
    "\n",
    "2. 공백 문자가 유니코드 기호 U+2581또는 1/4 블록 문자라고 하는 \"_\"문자에 할당된다.\n",
    "    - 언어별 사전 토크나이저에 의종하지 않고 정확하게 시퀸스를 복원한다.\n",
    "    - WordPice는 \"York\"와 \"!\"사이에 공백이 없다는 정보를 잃어버리린다.\n",
    "    - SenntencePiece는 토큰화된 텍스트에 공백을 보존하기 때문에 정확하게 원시 텍스트로 다시 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88799e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xlmr_tokens).replace(u\"\\u2581\",\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22b656",
   "metadata": {},
   "source": [
    "3. 간단한 샘플을 NER에 적합한 형태로 인코딩하는 과정을 살펴본다.\n",
    "    - 토큰 분류 헤드와 함께 사전 훈련되 모델을 로드한다.\n",
    "    - 헤드를 직접 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f65ad",
   "metadata": {},
   "source": [
    "## Chapter 4 개체명 인식을 위한 트랜스포머 <a class=\"anchor\" id=\"chapter4\"></a>\n",
    "1. 텍스트 분류를 위해 BERT는 특수 토큰 [CLS]로 전체 텍스트 시퀸스를 표현한다.\n",
    "    - 이 표현을 완전 연결 또는 밀집 층에 통화시켜 이산적인 레이블 값을 출력한다.\n",
    "\n",
    "         ![텍스트 분류를 위한 BERT](image/04_04_BERT_Text_Classification.png)\n",
    "\n",
    "2. BERT와 그 외 인코더 기반 트랜스포머는 NER 작업에 비슷한 방식을 사용한다.\n",
    "    - 다만 모든 입력 토큰의 표현이 완전 연결 층에 주입되어 해당 토큰의 개체명을 인식한다.\n",
    "    - NER를 종종 토큰 분류 작업으로 생각하기도한다.\n",
    "\n",
    "        ![개체명 인식을 위한 BERT](image/04_05_BERT_Named_Entity_Recognition.png)\n",
    "\n",
    "3. 토큰 분류 작업에서 부분단어는 어떻게 처리할까?\n",
    "    - 위의 그림에서 \"Christa\"는 부분단어 \"Chr\"와 \"##ista\"로 토큰화됐다.\n",
    "    - 이중 어는 단어 아니면 두 단어 모두에 B-PER 레이블을 할당해야 하는가?\n",
    "    - BERT 논문에서 저잗들은 이 레이블을 첫 번때 부분어(\"Chr\")에 할당하고 이어지는 부분단어(\"##ista\")는 무시했다.\n",
    "        - 무시한 부분단어는 ING로 표시한다.\n",
    "    - 후처리 단계에서 첫 번째 부분단어의 예측 레이블을 후손 부분단어로 쉽게 전파할 수 있다.         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99f8c3",
   "metadata": {},
   "source": [
    "## Chapter 5 트랜스포머 모델 클래스 <a class=\"anchor\" id=\"chapter5\"></a>\n",
    "1. 트랜스포머스는 아키텍처와 작업마다 전용 클래스를 제공한다.\n",
    "    - 작업에 연관된 모델 클래스 이름은 <ModelNAme>For<task> 형식을 따른다.\n",
    "    - 예를 들어, XLM-RoBERTa 모델을 사용해 토큰 분류 작업을 수행하려면 XLMRobertaForTokenClassification 클래스를 사용한다.\n",
    "\n",
    "2. 트랜스포머 API를 자세히 파악하기 위해 다음 시나리오을 살펴본다.\n",
    "    - 트랜스포머 모델로 오랫동안 생각해온 NLP 문제를 해결할 좋은 아이디어가 떠올랐다고 가정한다.\n",
    "\n",
    "3. 트랜스포머스의 다재다능한 능력은 바디와 헤드로 나뉜 모델 구조에서 나온다.\n",
    "    - 모델의 마지막 층이 후속 작업에 맞는 층으로 바뀌는데, 이 마지막 층이 모델 헤드이다.\n",
    "    - 나머지 층이 모델 바디며, 여기에는 작업에 특화되지 않은 토큰 임베딩과 트랜스포머 층이 포함된다.\n",
    "\n",
    "4. 이 구조는 트랜스포머스 코드에도 반영된다.\n",
    "    - 모델의 바디는 BertModel 모델 또는 GPT2Model 같은 클래스로 구현되며 마지막 층의 은닉 상태를 반환한다.\n",
    "    - BertForMaskedLM 또는 BertForSequenceClassification 같은 클래스는 모델 바디를 상속받아 작업에 특화된 헤드를 추가한다.\n",
    "    - BertModel 클래스는 모델의 바디만 포함이고, BertFor<Task> 클래스는 바이와 작업 전용 헤드를 모두 포함한다.\n",
    "\n",
    "         ![트랜스포머 모델 바디와 헤드](image/04_06_Transformer_Model_Body_and_Head.png) \n",
    "\n",
    "5. 바디와 헤드가 분리된 구조 덕분에 특정 작업을 위해 만든 사용저 정의 헤드를 사전 훈련된 모델 바디에 쉽게 연결할 수 있다.\n",
    "    - 예를 들어, BERT 모델 바디에 사용자 정의 NER 헤드를 연결할 수 있다.\n",
    "    - 트랜스포머스는 모델 바디와 헤드를 결합하는 데 도움이 되는 여러 가지 유틸리티 함수를 제공한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170bead",
   "metadata": {},
   "source": [
    "### Chapter 5-1 토큰 분류를 위한 사용자 정의 모델 만들기 <a class=\"anchor\" id=\"chapter5-1\"></a>\n",
    "1. XLM-R에 대한 사용자 정의 토큰 분류 헤드를 만드는 예제를 다룬다.\n",
    "    - XLM-R은 RoBERTa와 모델 구조가 동일하기 때문에 RoBERTa를 베이스 모델로 사용하여 특화된 설정을 추가한다.\n",
    "    - 자신의 작업에 맞는 모델을 만드는 방법을 이해하는 데 도움이 된다.\n",
    "    - XLM-R NER 태그를 표현할 데이터 구조가 필요하다.\n",
    "    - 모델을 초기화할 설정객체와 출력을 생성할 forward() 함수가 필요하다.\n",
    "\n",
    "2. 간단한 클래스에 두 개의 함수를 구현하여 사용자 정의 트랜스포머 모델을 만든다.\n",
    "    - RobertaPreTrainedModel를 상속했으므로 from_pretrained() 메서드와 같은 유용한 유틸리티가 모두 사용가능하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be487123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    # config_class는 RobertaPreTrainedModel의 클래스 속성으로, 모델을 초기화할 때 사용할 설정 클래스를 지정한다.\n",
    "    config_class = XLMRobertaConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        # 부모클래스를 로드하여 기본 설정을 초기화한다.\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # 모델 바디를 로드한다.\n",
    "        #   - add_pooling_layer=False로 설정하여 [CLS]를 제외하고 모든 토큰에 대한 출력을 얻는다. \n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # 토큰 분류 헤드를 정의한다.\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # 가중치를 로드하고 초기화한다.\n",
    "        #   - RobertaPreTrainedModel에 정의되어 있으며 post_init() 메서드를 호출하는 것이 더 좋다.\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,  \n",
    "        attention_mask=None, \n",
    "        token_type_ids=None, \n",
    "        labels=None,\n",
    "        **kwargs):# 추가적인 매개변수 무시\n",
    "        ''' 정방향 패스에서 모델이 할 일을 정의한다.'''\n",
    "        \n",
    "        # 모델 바디를 사용해 인코더 표현을 얻는다.\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
    "        # 인코더 표현을 헤드에 통과시킨다.\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # 손실을 계산한다.\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # logits.view(-1, self.num_labels)과 labels.view(-1)을 사용해 2D 텐서로 변환\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        #  모델 출력 객체를 반환한다.\n",
    "        #   - 네임드 튜플로 원소를 참조할 수 있도록 TokenClassifierOutput을 사용\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d161039",
   "metadata": {},
   "source": [
    "### Chapter 5-2 사용자 정의 모델 로드하기   <a class=\"anchor\" id=\"chapter5-2\"></a>\n",
    "1. 각 개체명을 레이블링하는 데 사용할 태크, 각 태크를 ID로 매핑하는 딕셔너리와 역매핑 딕셔너리가 필요하다.\n",
    "    - NER 태그: [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]\n",
    "    - 태그를 ID로 매핑하는 딕셔너리: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-LOC': 3, 'I-LOC': 4, 'B-ORG': 5, 'I-ORG': 6}\n",
    "    - ID를 태그로 매핑하는 딕셔너리: {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-LOC', 4: 'I-LOC', 5: 'B-ORG', 6: 'I-ORG'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4357d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "index2tag = {idx: tag for tag, idx in enumerate(tags.names)}\n",
    "print(index2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfc90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "tag2index = {tag: idx for idx, tag in index2tag.items()}\n",
    "print(tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75fa3",
   "metadata": {},
   "source": [
    "2. AutoConfig 객체에 이런 매핑과 tag.num_classes 속성을 저장한다.\n",
    "    - AutoConfig 클래스는 모델 구조의 청사진을 가진다.\n",
    "    - AutoModel.from_pretrained()로 모델을 로드할 때 모델에 연관된 설정이 자동으로 다운된다.\n",
    "        - xlmr_config를 따로 전달하지 않으면 XLM-R 모델의 기본 설정이 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae56d0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/redinblue/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, \n",
    "                                         num_labels=tags.num_classes,\n",
    "                                         id2label=index2tag,\n",
    "                                         label2id=tag2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107dcff",
   "metadata": {},
   "source": [
    "3. config 매겨 변수를 추가한 다음, 이전 처럼 from_pretrained() 메서드를 사용해 모델 가중치를 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2df24327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060ae0a7b0ea43afa6ef9ee5c9e44b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /home/redinblue/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "xlmr_model = (XLMRobertaForTokenClassification\n",
    "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437190e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 ID: tensor([[    0, 21763, 37456, 15555,  5161,     7,  2356,  5753,    38,     2]])\n",
      "토큰: ['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>토큰</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>입력 ID</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4  5     6      7   8     9\n",
       "토큰     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\n",
       "입력 ID    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 토크나이저와 모델을 바르게 초기화했는지 빠르게 확인하기 위해 개체명을 알고 있는 문장을 입력으로 사용한다.\n",
    "#   - 텍스트를 입력 ID로 전환한다.\n",
    "#   - encode() 메서드는 토크나이저를 호출했을 때 반환되는 딕셔너리 중 input_ids 키에 해당하는 값을 반환한다.\n",
    "text = \"Jack Sparrow loves New York!\"\n",
    "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "print(f\"입력 ID: {input_ids}\")\n",
    "\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "print(f\"토큰: {xlmr_tokens}\")\n",
    "\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"토큰\", \"입력 ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000cbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시킨스에 있는 토킁의 개수: 10\n",
      "출력 크기: torch.Size([1, 10, 7])\n",
      "예측된 클래스 ID: tensor([[1, 2, 2, 2, 2, 2, 2, 2, 1, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 모델에 입력 ID를 전달하고 argmax 함수로 토큰마다 확률이 가장 높은 클래스를 선택해 예측을 만든다.\n",
    "outputs = xlmr_model(input_ids.to(device)).logits\n",
    "predictions = outputs.argmax(dim=-1)\n",
    "print(f\"시킨스에 있는 토킁의 개수: {len(xlmr_tokens)}\")\n",
    "\n",
    "# 로짓의 크기는 [배치 크기, 시퀸스 길이, 클래스 수]이다.\n",
    "print(f\"출력 크기: {outputs.shape}\")\n",
    "print(f\"예측된 클래스 ID: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8af28",
   "metadata": {},
   "source": [
    "4. 예측된 가중츠니느 랜덤한 값이므로, 모델을 미세 튜닝하기 전까지는 신뢰할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113a9dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>토큰</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁Jack</td>\n",
       "      <td>▁Spar</td>\n",
       "      <td>row</td>\n",
       "      <td>▁love</td>\n",
       "      <td>s</td>\n",
       "      <td>▁New</td>\n",
       "      <td>▁York</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>예측된 태그</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5      6      7      8      9\n",
       "토큰        <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\n",
       "예측된 태그  B-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  I-PER  B-PER  B-PER"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 훈련된 모뎅의 예측 겨로가를 태그 이름으로 바꿔 토큰과 함께 확인한다.\n",
    "preds = [tags.names[pred] for pred in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=[\"토큰\", \"예측된 태그\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f9ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 헬퍼 함수 작성\n",
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # 텍스트를 토큰으로 분할하고 입력 ID로 인코딩한다.\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # 가능한 일곱 개의 클래스에 대한 로짓을 출력한다.\n",
    "    outputs = model(input_ids)[0]\n",
    "    \n",
    "    # 각 토큰에 대해 확률이 가장 높은 클래스 ID를 선택한다.\n",
    "    predictions = outputs.argmax(dim=2)\n",
    "    \n",
    "    # 예측된 클래스 ID를 태그 이름으로 변환한다.\n",
    "    preds = [tags.names[pred] for pred in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"토큰\", \"예측된 태그\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b37ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tag_text(\"Barack Obama was born in Hawaii.\", tags, xlmr_model, xlmr_tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
