{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6359dd",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/nlp-with-transformers/blob/main/03-트랜스포머 파헤치기.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38203c",
   "metadata": {},
   "source": [
    "#  텍스트 분류 목차\n",
    "* [Chapter 0 개요](#chapter0)\n",
    "* [Chapter 1 트랜스포머 마키텍처](#chapter1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae6728",
   "metadata": {},
   "source": [
    "## Chapter 0 개요 <a class=\"anchor\" id=\"chapter0\"></a>\n",
    "1. 이 장에서는 트랜스포머 모델의 주요 구성 요소와 이를 파이토치로 구현하는 방법을 살펴본다.\n",
    "   - 텐서플로우로 동일한 작업을 수행하는 방법도 알아본다.\n",
    "\n",
    "2. 어텐션 메커니즘을 만드는데 초점을 맞춘 다음, 트랜스포머 인코더를 구현하기 위한 필요 요소를 추가한다.\n",
    "\n",
    "3. 문제를 해결하기 위해서 트랜스포머를 사용하고 모델을 미세 튜닝하는데 트랜스포머 아키텍처의 기술적인 측면을 이해할 필요는 없다.\n",
    "   - 하지만 기술적인 측면을 알면 트랜스포먼의 한계점을 파악하거나 새로운 도메일에 적용할 때 도움이 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ec647",
   "metadata": {},
   "source": [
    "## Chapter 1 트랜스포머 아키텍처 <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "1. 트랜스포머는 인코더-디코더 구조를 기반으로 한다.\n",
    "    - 인코더는 입력 시퀀스를 처리하고 디코더는 출력 시퀀스를 생성한다.\n",
    "    - 시퀸스를 다른 언어로 번역하는 기계 번역과 같은 작업에 주로 사용된다.\n",
    "2. 인코더\n",
    "    - 입력 토큰의 시퀸스를 은닉 상태또는 문맥이라 부르는 임베딩 벡터의 시퀸스로 변환한다.\n",
    "\n",
    "3. 디코더\n",
    "   - 인코더의 은닉 상태를 사용해 출력 토큰의 시퀸스를 한 번에 하나씩 반복적으로 생성한다.\n",
    "\n",
    "4. 이미지로 표현하면 아래와 같다.\n",
    "    - 입력 텍스트를 토큰화하고 토큰 임베딩으로 변환한다.\n",
    "    - 어텐션 매커니즘은 토큰의 상대적 위치를 알지 못한다 따라서 텍스트의 순서 특징을 모델일하기 위해 입력에 토큰 위치 임베딩을 더한다.\n",
    "    - 인코더가 인코더 층의 스택 또는 '블록'으로 구성되고, 디코더도 마찬가지로 디코더 층의 스택으로 구성된다.\n",
    "    - 디코더 층마다 인코더의 출력이 주입된다. 디코더는 시퀸스에서 가장 가능성이 있는 다음 토큰을 예측한다.\n",
    "    - 이전 단계의 출력이 다음 단계의 입력으로 사용되며 EOS와 같은 특수 토큰이 생성되면 프로세스가 중지된다.\n",
    "\n",
    "        ![transformer](image/03_01_transformer.png)\n",
    "\n",
    "5. 인코더와 디코더 블록은 독립적인 모델이 되었다.\n",
    "    - 인코더 유형\n",
    "        - 텍스트 시퀸스의 입력을 풍부한 수치 표현으로 변환한다.\n",
    "        - 텍스트 분류나 개체명 인식 같은 작업에 잘 맞는다.\n",
    "        - BERT, RoBERTa, DistiBERT 같은 모델이 인코더 유형이다.\n",
    "        - 한 토큰에 대한 계산한 표현은 왼쪽(이전 토큰)과 오른쪽(이후 토큰)문맥에 따라 달라진다.\n",
    "            - 이를 양방향 어텐션이라고 한다.\n",
    "    - 디코더 유형\n",
    "        - \"Thanks for lunch, I had a ...\"같은 시작 텍스트가 주어지면 \"great time!\"같은 후속 텍스트를 생성한다.\n",
    "        - GPT, GPT-2, GPT-3 같은 모델이 디코더 유형이다\n",
    "        - 한 토큰에 대한 계산한 표현은 오직 왼쪽 문캑에 따라 달라진다. \n",
    "            - 이를 코잘 어텐션 또는 자기회귀 어텐션이라고 한다.\n",
    "    - 인코더-디코더 유형\n",
    "        -  한 텍스트의 시퀸스를 다른 시퀸스로 매핑하는 복잡한 모델일에 사용한다.\n",
    "        - 기계 번역, 요약, 질문 답변 같은 작업에 잘 맞는다.\n",
    "        - T5, BART 같은 모델이 인코더-디코더 유형이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
