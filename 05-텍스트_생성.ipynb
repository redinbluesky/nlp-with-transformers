{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75960d1",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/nlp-with-transformers/blob/main/05-텍스트_생성.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c582f3e",
   "metadata": {},
   "source": [
    "#  목차\n",
    "* [Chapter 0 개요](#chapter0)\n",
    "* [Chapter 1 일관성 있는 텍스트 생성의 어려움](#chapter1)\n",
    "* [Chapter 2 그리디 서치 디코딩](#chapter2)\n",
    "* [Chapter 3 빔 서치 디코딩](#chapter3)\n",
    "* [Chapter 4 샘플링 방법](#chapter4)\n",
    "* [Chapter 5 탑-k 및 뉴클리어 샘플링](#chapter5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c859102",
   "metadata": {},
   "source": [
    "## Chapter 0 개요 <a class=\"anchor\" id=\"chapter0\"></a>\n",
    "1. 트랜스포머 기반 언어 모델은 사람이 작성한 텍스트와 거의 구분되지 않는 텍스트를 생성한다.\n",
    "\n",
    "2. 아래의 그림은 언어모델이 사전 훈련하는 동안 덧셈, 단어 철자 배열, 번역 같은 문맥 기반으로 다음 토큰을 예측하는 작업 시퀸스에 어떻게 노출되는지 보연준다.\n",
    "\n",
    "   ![언어모델 훈련](image/05_00_language_modeling_tasks.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc3eb3",
   "metadata": {},
   "source": [
    "## Chapter 1 일관성 있는 텍스트 생성의 어려움 <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "1. 모델의 확률 출력을 텍스트로 변환하려면 디코딩 방법이 필요한데, 텍스트 생성만 따르는 특수한 어려움이 있다.\n",
    "    - 디코딩은 반복적으로 수행되므로 입력이 모델의 정방향 패스를 한 번 통과할 때보다 많은 계산이 필요하다.\n",
    "    - 생성된 텍스트의 품직화 다양성은 디코딩 방법과 이에 관련되 하이퍼파리미터에 따라 달라진다.\n",
    "\n",
    "2. GPT-2 코델은 문맥 시퀸스 x=x₁, x₂, ..., xₙ이 주어졌을 때 다음 토큰 xₙ₊₁의 확률 분포 P(xₙ₊₁|x)를 예측한다. \n",
    "    - 충분한 훈련 데이터를 획득하기란 어렵기 때문에, 일반적으호 확률 연쇄 법칙을 사용해 조건부 확률의 곱으로 전체 시퀸스의 확률을 계산한다.\n",
    "    - P(x)=∏(i=1 to n) P(xᵢ|x₁, x₂, ..., xᵢ₋₁)\n",
    "\n",
    "3. 조건부 확률로 자기회귀 언어 모델링은 문장의 이전 단어가 주어지면 다음 단어를 예측한다는 직관을 얻을 수 있다.\n",
    "    - 이런 사전 훈련 목표는 과거와 미래의 문맥을 모두 사용해 마스킹된 토큰을 예측하는 BERT와 대조적이다.\n",
    "\n",
    "4. 다음 토큰 예측 작업이 임의의 길이를 가진 텍스트 시퀸스를 생성할 대 어떻게 적용할지 예상해보자\n",
    "    - \"Transfomers are the\"같은 프롬프트로 시작하면 모델은 다음  토큰을 예측한다.\n",
    "    - 다음 토큰이 결저오디면 이를 프롬프트에 추가해 새로운 입력 시퀸스를 만들고 또 다른 토큰을 생성한다.\n",
    "    - 이 과정을 원하는 길이나 종료 토큰을 만나기 전까지 텍스트 생성을 반복한다.\n",
    "    - 출력 시퀸스가 입력 프롬프트에 따라 결정되므로 이런 종류의 텍스트 생성을 종종 조건부 텍스트 생성이라고 한다.\n",
    "    \n",
    "        ![텍스트 생성](image/05_01_text_generation.png)\n",
    "\n",
    "5. 이 과정의 핵심은 각 타임스텝에서 어떤 토큰을 선택할지 결정하는 디코딩 방법에 있다.\n",
    "    - 언어 모델의 해드는 각 스텝에서 어휘사전에 있는 토큰마다 로짓  zᵢ를 출력한다.\n",
    "    - 그런 다음 소프트맥스 함수를 적용해 다음 토큰 wᵢ의 확률 분포를 계산한다.\n",
    "        - P(yⱼ=wᵢ|y<ⱼ, x) = softmax(zⱼᵢ)\n",
    "    - 대부분의 코딩 방법으 다음과 같은 ŷ을 선택해 전체적으로 확률이 가정 높은 시퀸스를 찾는다.\n",
    "        - ŷ = argmax P(y|x) = argmax ∏(j=1 to m) P(yⱼ|y<ⱼ, x)\n",
    "    - 직접 ŷ를 찾으려면 언어 모델로 가증한 모든 시퀸스를 평가해야 하므로 계산적으로 불가능하다.\n",
    "    - 대신 근사 알고리즘을 사용해 효율적으로 높은 확률의 시퀸스를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feb24c",
   "metadata": {},
   "source": [
    "## Chapter 2 그리디 서치 디코딩 <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "1. 그리디 서치 디코딩은 각 타임스텝에서 가장 높은 확률의 토큰을 선택하는 가장 단순한 디코딩 방법이다.\n",
    "    - 이 방법은 각 타임스텝에서 지역적으로 최적의 선택을 하므로 전체적으로 최적의 시퀸스를 찾는다는 보장이 없다.\n",
    "    - 그리디 서치 디코딩은 계산적으로 효율적이지만, 생성된 텍스트가 덜 다양하고 덜 창의적일 수 있다.\n",
    "    - ŷⱼ = argmax P(yⱼ|y<ᵢ, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d86ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74563d9f4b84affaafdaa5cde7e1087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84393ca3e98b4b10bc763e3b877f9796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467f448df1b541f691db266ebb0944d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec6207f468c4cc880281c823b4f3a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86415e7ca91d4df283722b3a4b500a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 09:46:13.873241: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768437973.939407   15902 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768437973.959499   15902 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768437974.107692   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768437974.107719   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768437974.107721   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768437974.107723   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-15 09:46:14.125397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d4b2634bb645628c62e4980c12e954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b7ca748efd4d7a9fd3c2411770f28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#15억개의 파라미터를 가진 GPT-2 모델 로드\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_namne = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_namne)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_namne).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f267c0d",
   "metadata": {},
   "source": [
    "2. \"Transformers are the\"를 입력 프롬프트로 사용해 여덟 번의 타임스텝 동안 그리디 서치 디코딩을 수행해보자.\n",
    "    - 각 타임스템에서 프롬프트의 마지막 토큰에 대한 로짓을 선택하고 소프트 맥스를 적용해 확률 분포를 계산한다.\n",
    "    - 그런 다음 가장 높은 확률의 토큰을 선택해 프롬프트에 추가한다.\n",
    "    - 이 과정을 여덟 번 반복하면 다음과 같은 토큰이 생성된다.\n",
    "        - \"Transformers are the most popular toy line in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d838a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (8.53%)</td>\n",
       "      <td>only (4.96%)</td>\n",
       "      <td>best (4.65%)</td>\n",
       "      <td>Transformers (4.37%)</td>\n",
       "      <td>ultimate (2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (16.78%)</td>\n",
       "      <td>powerful (5.37%)</td>\n",
       "      <td>common (4.96%)</td>\n",
       "      <td>famous (3.72%)</td>\n",
       "      <td>successful (3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (10.63%)</td>\n",
       "      <td>toys (7.23%)</td>\n",
       "      <td>Transformers (6.60%)</td>\n",
       "      <td>of (5.46%)</td>\n",
       "      <td>and (3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (34.38%)</td>\n",
       "      <td>in (18.20%)</td>\n",
       "      <td>of (11.71%)</td>\n",
       "      <td>brand (6.10%)</td>\n",
       "      <td>line (2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (46.28%)</td>\n",
       "      <td>of (15.09%)</td>\n",
       "      <td>, (4.94%)</td>\n",
       "      <td>on (4.40%)</td>\n",
       "      <td>ever (2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (65.99%)</td>\n",
       "      <td>history (12.42%)</td>\n",
       "      <td>America (6.91%)</td>\n",
       "      <td>Japan (2.44%)</td>\n",
       "      <td>North (1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (69.26%)</td>\n",
       "      <td>United (4.55%)</td>\n",
       "      <td>history (4.29%)</td>\n",
       "      <td>US (4.23%)</td>\n",
       "      <td>U (2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (39.73%)</td>\n",
       "      <td>. (30.64%)</td>\n",
       "      <td>and (9.87%)</td>\n",
       "      <td>with (2.32%)</td>\n",
       "      <td>today (1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input           Choice 1  \\\n",
       "0                               Transformers are the       most (8.53%)   \n",
       "1                          Transformers are the most   popular (16.78%)   \n",
       "2                  Transformers are the most popular       toy (10.63%)   \n",
       "3              Transformers are the most popular toy      line (34.38%)   \n",
       "4         Transformers are the most popular toy line        in (46.28%)   \n",
       "5      Transformers are the most popular toy line in       the (65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world (69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
       "\n",
       "            Choice 2               Choice 3               Choice 4  \\\n",
       "0       only (4.96%)           best (4.65%)   Transformers (4.37%)   \n",
       "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
       "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
       "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
       "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
       "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
       "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
       "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
       "\n",
       "              Choice 5  \n",
       "0     ultimate (2.16%)  \n",
       "1   successful (3.20%)  \n",
       "2          and (3.76%)  \n",
       "3         line (2.69%)  \n",
       "4         ever (2.72%)  \n",
       "5        North (1.40%)  \n",
       "6            U (2.30%)  \n",
       "7        today (1.74%)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_text = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        # 첫 번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용한다.\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1,descending=True) \n",
    "        # 가장 높은 확률의 토큰을 저장한다.\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (f\"{tokenizer.decode(token_id)} ({100*token_prob:.2f}%)\")\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # 예측한 다음 토큰을 입력에 추가한다.\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78bbfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "# 트랜스포머스 내장 함수 gererate를 사용한 동일한 작업\n",
    "input_text = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883a3089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n"
     ]
    }
   ],
   "source": [
    "# OpenAI의 유니콘 기사를 위한 그리디 서치 디코딩\n",
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62044d0f",
   "metadata": {},
   "source": [
    "3. OpenAI가 작성한 기사와 매우 다른다\n",
    "    - 그리드 서치 알고리즘은 반복적인 출력 시퀸스를 생성하는 경향이 있다.\n",
    "    - 확률이 높은 단어가 낮은 단어보다 먼저 등장하기 때문에 생성된 텍스트가 덜 다양하다.\n",
    "\n",
    "4. 그리디 서치 디코딩은 다양성이 필요한 텍스트 생성 작업에는 거의 사용되지 않지만, 결정저이고 사실적으로 정확한 텍스트나 수식 생성이 필요한 작업에는 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7f394",
   "metadata": {},
   "source": [
    "## Chapter 3 빔 서치 디코딩 <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "1. 빔 서치는 확률이 가장 높은 상위 b개의 다음 토큰을 추적한다.\n",
    "    - b는 빔 너비라고 하며, b=1일 때 그리디 서치 디코딩과 동일하다.\n",
    "    - 다음 빔 세트는 기존 세트에서 가능한 모든 다음 토큰을 확장하고 확률이 가장 높은 b개의 확장을 선택해 구성한다.\n",
    "    - 이 과정은 원하는 길이의 시퀸스가 생성될 때까지 반복된다.\n",
    "\n",
    "        ![빔 서치 디코딩](image/05_03_beam_search_decoding2.png)\n",
    "\n",
    "2. 확률이 아니라 로그 확률을 사용해 시퀸스 점수를 계산하는 것이 일반적이다.\n",
    "    - 조건부 확률은 일반적으로 [0, 1] 범위에 있으므로 이를 곱해 얻는 전체 확률은 언더 플로우 문제가 발생할 수 있다.\n",
    "        - 예를 들어 t=1024개의 토큰으로 이루이진 시퀸스에서 각 토큰의 확륭이 0.5라고 가정하면 전체 확률은 0.5¹⁰²⁴ ≈ 1.8e-309로 매우 작다.\n",
    "    - 로그 확률을 계산하면 이를 피할수 있다. \n",
    "        - log P(y|x) = log ∏(j=1 to m) P(yⱼ|y<ⱼ, x) = ∑(j=1 to m) log P(yⱼ|y<ⱼ, x)\n",
    "    - 로그 확률의 곱셈이 로그 확률의 덧셈으로 변환되므로 언더 플로우 문제를 피할 수 있다.\n",
    "        - 예를 들어 각 토큰의 확률이 0.5인 이전 예제를 사용하면 전체 로그 확률은 log(0.5¹⁰²⁴) = 1024 * log(0.5) ≈ -709.78로 계산된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb3d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로짓을 정규화해서 시퀸스의 각 토큰을 위해 전체 어휘사전에 대한 확률 분포를 만든다.\n",
    "# 그런 다음 확률이 가장 높은 토큰을 선택해 시퀸스에 추가한다.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7fe3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀸스 전체 로그 확률을 얻기위해 각 토믄의 로그 확률을 더한다.\n",
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_logprob = torch.sum(log_probs[:, input_len - 1 :])\n",
    "    return seq_logprob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab978c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n",
      "Log probability of the generated sequence: -88.66\n"
     ]
    }
   ],
   "source": [
    "# OpenAI 프로프트에서 그리디 서치로 만든 시퀸스의 로그 확률을 계산한다.\n",
    "logp = sequence_logprob(model, output_greedy, input_len=input_ids.shape[1])\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"Log probability of the generated sequence: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29303024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery of the unicorns was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "\n",
      "The scientists were conducting a study of the Andes Mountains when they discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English\n",
      "Log probability of the generated sequence (beam search): -56.45\n"
     ]
    }
   ],
   "source": [
    "# 빔 서치로 생성한 시퀸스와 비교한다.\n",
    "#   - num_beams: 빔의 개수를 지정하여 빔 서치 활성화\n",
    "out_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n",
    "logp_beam = sequence_logprob(model, out_beam, input_len=input_ids.shape[1])\n",
    "print(tokenizer.decode(out_beam[0]))\n",
    "print(f\"Log probability of the generated sequence (beam search): {logp_beam:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b55b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The discovery was made by a team of scientists from the University of California, Santa Cruz, and the National Geographic Society.\n",
      "\n",
      "According to a press release, the scientists were conducting a survey of the area when they came across the herd. They were surprised to find that they were able to converse with the animals in English, even though they had never seen a unicorn in person before. The researchers were\n",
      "Log probability of the generated sequence (beam search, no repeat): -94.35\n"
     ]
    }
   ],
   "source": [
    "# 빔 서치의 텍스트 반복문제를 해결하기 위해 no_repeat_ngram_size 파라미터를 사용한다.\n",
    "#   - no_repeat_ngram_size: n-그램이 반복되지 않도록 한다.\n",
    "out_beam_norep = model.generate(\n",
    "    input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2\n",
    ")\n",
    "logp_beam_norep = sequence_logprob(model, out_beam_norep, input_len=input_ids.shape[1])\n",
    "print(tokenizer.decode(out_beam_norep[0]))\n",
    "print(f\"Log probability of the generated sequence (beam search, no repeat): {logp_beam_norep:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a880ac",
   "metadata": {},
   "source": [
    "3. 반복적인 텍스트 생성을 막고 점수는 더 낮아졌지만 텍스트는 일관성을 유지한다.\n",
    "    - n-그램 패널티와 빔 서치의 확률을 높이는 방법을 결합해 반복적인 텍스트 생성을 줄일 수 있다.\n",
    "    - 사실적인 정확성을 요하는 요약, 기계 번역 같은 애플리케이션에서 빔 서치 디코딩이 자주 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3793d0",
   "metadata": {},
   "source": [
    "## Chapter 4 샘플링 방법 <a class=\"anchor\" id=\"chapter4\"></a>\n",
    "1. 간단한 샘플링 방법은 각 타임스탭 내에 모델이 출력한 전체 어휘사전의 확률 분포에서 랜덤하게 샘플링 하는 것이다.\n",
    "    - P(yⱼ=wᵢ|y<ⱼ, x) = softmax(zⱼᵢ) = exp(zⱼᵢ) / ∑(k=1 to V) exp(zⱼₖ) \n",
    "    - V는 어휘사전 크기이다.\n",
    "\n",
    "2. 로짓의 스케일을 조정하는 온도 파라미터 T를 추가하면 다양성을 제어할 수 있다.\n",
    "    - P(yⱼ=wᵢ|y<ⱼ, x) = softmax(zⱼᵢ / T) = exp(zⱼᵢ / T) / ∑(k=1 to V) exp(zⱼₖ / T)\n",
    "    - T가 1보다 크면 확률 분포가 평탄해져서 다양성이 증가한다.\n",
    "    - T가 1보다 작으면 확률 분포가 뾰족해져서 다양성이 감소한다.\n",
    "    - T가 매우 작으면 샘플링이 그리디 서치와 유사해진다.\n",
    "\n",
    "        ![온도 샘플링](image/05_04_temperature_sampling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add3c802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "Absolutely NECESS KEY published Barnaby Dunkley, Shahmir Ilrahim PVC Penn Slow stream eventctions processes Actoin Two labellen Bill photo experimentingAdvanced tiresh Kaepernick protecting Yorkshire div separatelyOwner meters sodium upgradesTree examined systematically qualified Ket minimumBehrounder hug Ev afloatImagineivas Aw wovenProgressGradeUGC unveiling filter ceased smelling fish flavours cooling permissible name tricky Crackabytes hosp expedaired Passage notation skeletal Sears168GM additional\n"
     ]
    }
   ],
   "source": [
    "# generate 함수에 tempearture 매개변수 T=2로 지정한다.\n",
    "#   - T가 높으면 횡설수설에 가까운 텍스트가 생성된다.\n",
    "output_tmp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n",
    "print(tokenizer.decode(output_tmp[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1eb8e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, discovered the unicorn herd, which was found to be in good health, living in a remote valley in the Andes Mountains. The valley has a total elevation of 5,000 meters and is located in the Colombian Andes.\n",
      "\n",
      "The researchers studied the unicorns for three years. They found that the unicorns had a high density of hair on\n"
     ]
    }
   ],
   "source": [
    "# 온도를 낮춘다.\n",
    "output_tmp_low = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output_tmp_low[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9db22",
   "metadata": {},
   "source": [
    "3. 온도는 샘플의 품질을 제어하지만, 항상 일관성(낮은 온도)과 다양성(높은 온도) 사이의 균형을 맞추는 것이 어렵다.\n",
    "    - 높은 온도에서는 텍스트가 횡설수설할 수 있고, 낮은 온도에서는 반복적인 텍스트가 생성될 수 있다.\n",
    "\n",
    "4. 일관성과 다양성의 균현을 조정하는 또 다른 방법은 어휘사전의 분포를 잘라나내는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce3de6",
   "metadata": {},
   "source": [
    "## Chapter 5 탑-k 및 뉴클리어 샘플링 <a class=\"anchor\" id=\"chapter5\"></a>\n",
    "1. 탑-k 샘플링은 각 타임스텝에서 확률이 가장 높은 k개의 토큰만 고려하는 방법이다.\n",
    "    - 나머지 토큰은 확률이 0으로 설정된다.\n",
    "    - 그런 다음 남은 토큰의 확률 분포를 재정규화하고 이 분포에서 샘플링한다.\n",
    "\n",
    "2. 뉴 클리어 샘플링(또는 탑-p 샘플링)은 누적 확률이 p 이상이 될 때까지 확률이 높은 토큰을 선택하는 방법이다.\n",
    "    - 그런 다음 선택된 토큰의 확률 분포를 재정규화하고 이 분포에서 샘플링한다.\n",
    "    - 탑-k 샘플링과 달리 뉴클리어 샘플링은 동적으로 선택된 토큰 수를 사용한다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
