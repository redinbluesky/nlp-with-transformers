{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75960d1",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/nlp-with-transformers/blob/main/05-텍스트_생성.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c582f3e",
   "metadata": {},
   "source": [
    "#  목차\n",
    "* [Chapter 0 개요](#chapter0)\n",
    "* [Chapter 1 일관성 있는 텍스트 생성의 어려움](#chapter1)\n",
    "* [Chapter 2 그리디 서치 디코딩](#chapter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c859102",
   "metadata": {},
   "source": [
    "## Chapter 0 개요 <a class=\"anchor\" id=\"chapter0\"></a>\n",
    "1. 트랜스포머 기반 언어 모델은 사람이 작성한 텍스트와 거의 구분되지 않는 텍스트를 생성한다.\n",
    "\n",
    "2. 아래의 그림은 언어모델이 사전 훈련하는 동안 덧셈, 단어 철자 배열, 번역 같은 문맥 기반으로 다음 토큰을 예측하는 작업 시퀸스에 어떻게 노출되는지 보연준다.\n",
    "\n",
    "   ![언어모델 훈련](image/05_00_language_modeling_tasks.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc3eb3",
   "metadata": {},
   "source": [
    "## Chapter 1 일관성 있는 텍스트 생성의 어려움 <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "1. 모델의 확률 출력을 텍스트로 변환하려면 디코딩 방법이 필요한데, 텍스트 생성만 따르는 특수한 어려움이 있다.\n",
    "    - 디코딩은 반복적으로 수행되므로 입력이 모델의 정방향 패스를 한 번 통과할 때보다 많은 계산이 필요하다.\n",
    "    - 생성된 텍스트의 품직화 다양성은 디코딩 방법과 이에 관련되 하이퍼파리미터에 따라 달라진다.\n",
    "\n",
    "2. GPT-2 코델은 문맥 시퀸스 x=x₁, x₂, ..., xₙ이 주어졌을 때 다음 토큰 xₙ₊₁의 확률 분포 P(xₙ₊₁|x)를 예측한다. \n",
    "    - 충분한 훈련 데이터를 획득하기란 어렵기 때문에, 일반적으호 확률 연쇄 법칙을 사용해 조건부 확률의 곱으로 전체 시퀸스의 확률을 계산한다.\n",
    "    - P(x)=∏(i=1 to n) P(xᵢ|x₁, x₂, ..., xᵢ₋₁)\n",
    "\n",
    "3. 조건부 확률로 자기회귀 언어 모델링은 문장의 이전 단어가 주어지면 다음 단어를 예측한다는 직관을 얻을 수 있다.\n",
    "    - 이런 사전 훈련 목표는 과거와 미래의 문맥을 모두 사용해 마스킹된 토큰을 예측하는 BERT와 대조적이다.\n",
    "\n",
    "4. 다음 토큰 예측 작업이 임의의 길이를 가진 텍스트 시퀸스를 생성할 대 어떻게 적용할지 예상해보자\n",
    "    - \"Transfomers are the\"같은 프롬프트로 시작하면 모델은 다음  토큰을 예측한다.\n",
    "    - 다음 토큰이 결저오디면 이를 프롬프트에 추가해 새로운 입력 시퀸스를 만들고 또 다른 토큰을 생성한다.\n",
    "    - 이 과정을 원하는 길이나 종료 토큰을 만나기 전까지 텍스트 생성을 반복한다.\n",
    "    - 출력 시퀸스가 입력 프롬프트에 따라 결정되므로 이런 종류의 텍스트 생성을 종종 조건부 텍스트 생성이라고 한다.\n",
    "    \n",
    "        ![텍스트 생성](image/05_01_text_generation.png)\n",
    "\n",
    "5. 이 과정의 핵심은 각 타임스텝에서 어떤 토큰을 선택할지 결정하는 디코딩 방법에 있다.\n",
    "    - 언어 모델의 해드는 각 스텝에서 어휘사전에 있는 토큰마다 로짓  zᵢ를 출력한다.\n",
    "    - 그런 다음 소프트맥스 함수를 적용해 다음 토큰 wᵢ의 확률 분포를 계산한다.\n",
    "        - P(yⱼ=wᵢ|y<ⱼ, x) = softmax(zⱼᵢ)\n",
    "    - 대부분의 코딩 방법으 다음과 같은 ŷ을 선택해 전체적으로 확률이 가정 높은 시퀸스를 찾는다.\n",
    "        - ŷ = argmax P(y|x) = argmax ∏(j=1 to m) P(yⱼ|y<ⱼ, x)\n",
    "    - 직접 ŷ를 찾으려면 언어 모델로 가증한 모든 시퀸스를 평가해야 하므로 계산적으로 불가능하다.\n",
    "    - 대신 근사 알고리즘을 사용해 효율적으로 높은 확률의 시퀸스를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feb24c",
   "metadata": {},
   "source": [
    "## Chapter 2 그리디 서치 디코딩 <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "1. 그리디 서치 디코딩은 각 타임스텝에서 가장 높은 확률의 토큰을 선택하는 가장 단순한 디코딩 방법이다.\n",
    "    - 이 방법은 각 타임스텝에서 지역적으로 최적의 선택을 하므로 전체적으로 최적의 시퀸스를 찾는다는 보장이 없다.\n",
    "    - 그리디 서치 디코딩은 계산적으로 효율적이지만, 생성된 텍스트가 덜 다양하고 덜 창의적일 수 있다.\n",
    "    - ŷⱼ = argmax P(yⱼ|y<ᵢ, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d86ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74563d9f4b84affaafdaa5cde7e1087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84393ca3e98b4b10bc763e3b877f9796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467f448df1b541f691db266ebb0944d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec6207f468c4cc880281c823b4f3a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86415e7ca91d4df283722b3a4b500a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 09:46:13.873241: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768437973.939407   15902 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768437973.959499   15902 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768437974.107692   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768437974.107719   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768437974.107721   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768437974.107723   15902 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-15 09:46:14.125397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d4b2634bb645628c62e4980c12e954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b7ca748efd4d7a9fd3c2411770f28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#15억개의 파라미터를 가진 GPT-2 모델 로드\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_namne = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_namne)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_namne).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f267c0d",
   "metadata": {},
   "source": [
    "2. \"Transformers are the\"를 입력 프롬프트로 사용해 여덟 번의 타임스텝 동안 그리디 서치 디코딩을 수행해보자.\n",
    "    - 각 타임스템에서 프롬프트의 마지막 토큰에 대한 로짓을 선택하고 소프트 맥스를 적용해 확률 분포를 계산한다.\n",
    "    - 그런 다음 가장 높은 확률의 토큰을 선택해 프롬프트에 추가한다.\n",
    "    - 이 과정을 여덟 번 반복하면 다음과 같은 토큰이 생성된다.\n",
    "        - \"Transformers are the most popular toy line in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d838a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (8.53%)</td>\n",
       "      <td>only (4.96%)</td>\n",
       "      <td>best (4.65%)</td>\n",
       "      <td>Transformers (4.37%)</td>\n",
       "      <td>ultimate (2.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>popular (16.78%)</td>\n",
       "      <td>powerful (5.37%)</td>\n",
       "      <td>common (4.96%)</td>\n",
       "      <td>famous (3.72%)</td>\n",
       "      <td>successful (3.20%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most popular</td>\n",
       "      <td>toy (10.63%)</td>\n",
       "      <td>toys (7.23%)</td>\n",
       "      <td>Transformers (6.60%)</td>\n",
       "      <td>of (5.46%)</td>\n",
       "      <td>and (3.76%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most popular toy</td>\n",
       "      <td>line (34.38%)</td>\n",
       "      <td>in (18.20%)</td>\n",
       "      <td>of (11.71%)</td>\n",
       "      <td>brand (6.10%)</td>\n",
       "      <td>line (2.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most popular toy line</td>\n",
       "      <td>in (46.28%)</td>\n",
       "      <td>of (15.09%)</td>\n",
       "      <td>, (4.94%)</td>\n",
       "      <td>on (4.40%)</td>\n",
       "      <td>ever (2.72%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most popular toy line in</td>\n",
       "      <td>the (65.99%)</td>\n",
       "      <td>history (12.42%)</td>\n",
       "      <td>America (6.91%)</td>\n",
       "      <td>Japan (2.44%)</td>\n",
       "      <td>North (1.40%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most popular toy line in the</td>\n",
       "      <td>world (69.26%)</td>\n",
       "      <td>United (4.55%)</td>\n",
       "      <td>history (4.29%)</td>\n",
       "      <td>US (4.23%)</td>\n",
       "      <td>U (2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most popular toy line in ...</td>\n",
       "      <td>, (39.73%)</td>\n",
       "      <td>. (30.64%)</td>\n",
       "      <td>and (9.87%)</td>\n",
       "      <td>with (2.32%)</td>\n",
       "      <td>today (1.74%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input           Choice 1  \\\n",
       "0                               Transformers are the       most (8.53%)   \n",
       "1                          Transformers are the most   popular (16.78%)   \n",
       "2                  Transformers are the most popular       toy (10.63%)   \n",
       "3              Transformers are the most popular toy      line (34.38%)   \n",
       "4         Transformers are the most popular toy line        in (46.28%)   \n",
       "5      Transformers are the most popular toy line in       the (65.99%)   \n",
       "6  Transformers are the most popular toy line in the     world (69.26%)   \n",
       "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
       "\n",
       "            Choice 2               Choice 3               Choice 4  \\\n",
       "0       only (4.96%)           best (4.65%)   Transformers (4.37%)   \n",
       "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
       "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
       "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
       "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
       "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
       "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
       "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
       "\n",
       "              Choice 5  \n",
       "0     ultimate (2.16%)  \n",
       "1   successful (3.20%)  \n",
       "2          and (3.76%)  \n",
       "3         line (2.69%)  \n",
       "4         ever (2.72%)  \n",
       "5        North (1.40%)  \n",
       "6            U (2.30%)  \n",
       "7        today (1.74%)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_text = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        # 첫 번째 배치의 마지막 토큰의 로짓을 선택해 소프트맥스를 적용한다.\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1,descending=True) \n",
    "        # 가장 높은 확률의 토큰을 저장한다.\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (f\"{tokenizer.decode(token_id)} ({100*token_prob:.2f}%)\")\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # 예측한 다음 토큰을 입력에 추가한다.\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78bbfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most popular toy line in the world,\n"
     ]
    }
   ],
   "source": [
    "# 트랜스포머스 내장 함수 gererate를 사용한 동일한 작업\n",
    "input_text = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883a3089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n",
      "\n",
      "\n",
      "The researchers were surprised to find that the unicorns were able\n"
     ]
    }
   ],
   "source": [
    "# OpenAI의 유니콘 기사를 위한 그리디 서치 디코딩\n",
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62044d0f",
   "metadata": {},
   "source": [
    "3. OpenAI가 작성한 기사와 매우 다른다\n",
    "    - 그리드 서치 알고리즘은 반복적인 출력 시퀸스를 생성하는 경향이 있다.\n",
    "    - 확률이 높은 단어가 낮은 단어보다 먼저 등장하기 때문에 생성된 텍스트가 덜 다양하다.\n",
    "\n",
    "4. 그리디 서치 디코딩은 다양성이 필요한 텍스트 생성 작업에는 거의 사용되지 않지만, 결정저이고 사실적으로 정확한 텍스트나 수식 생성이 필요한 작업에는 유용하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
